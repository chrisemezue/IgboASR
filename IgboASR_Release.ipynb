{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IgboASR Release.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibD6bsRPl8Qu"
      },
      "source": [
        "# Building an end-to-end Speech Recognition model in PyTorch for Igbo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7I-IEtzFltf"
      },
      "source": [
        "## Welcome to this notebook, where we will walk you through building an end-to-end speech recognition model in Pytorch for Igbo language. This is part of the [OkwuGbe paper](https://arxiv.org/abs/2103.07762) on building ASR models for Fon and Igbo\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx3c3zAZFLlj"
      },
      "source": [
        "This code was inspired by the article [Building an end-to-end Speech Recognition model in PyTorch](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch)\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac_xXszFGFIh"
      },
      "source": [
        "We connect to Google drive. We provide the directory to save the model checkpoints, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joPyp2MirIkN",
        "outputId": "c05fbc55-4696-44d8-ae78-091c2e0dd48e"
      },
      "source": [
        "import os\r\n",
        "#Connect to Gdrive to store model checkpoints\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "model_path = '/content/drive/MyDrive/IgboASR'\r\n",
        "if not os.path.isdir(model_path):\r\n",
        "  os.makedirs(model_path)\r\n",
        "model_path = '/content/drive/MyDrive/IgboASR/ig_asr'\r\n",
        "model_path_loss = '/content/drive/MyDrive/IgboASR/ig_asr_best_loss' \r\n",
        "#model_path='./ig_asr'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU_-oWXrGpre"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5Rbt8cCGSQz"
      },
      "source": [
        "Install dependencies.\r\n",
        "\r\n",
        "\r\n",
        "> Torch audio v 0.4.0\r\n",
        "\r\n",
        "> Torch v 1.4.0\r\n",
        "\r\n",
        "> gdown is used to access Google drive capabilities. Like automatically downloading your dataset, saved on your GDrive straight to your workspace.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwfN8o17Bdp2"
      },
      "source": [
        "!pip install torchaudio==0.4.0 torch==1.4.0 > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z83Y9vFjUNi"
      },
      "source": [
        "!pip install gdown >/dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rdj561RcLMd"
      },
      "source": [
        "#All imports\r\n",
        "\r\n",
        "import os,sys,re\r\n",
        "import unicodedata #to normalize diacritics\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.utils.data as data\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "import torchaudio\r\n",
        "import numpy as np\r\n",
        "import zipfile\r\n",
        "import random\r\n",
        "from typing import Tuple\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX0azmI-GtN1"
      },
      "source": [
        "This is to download the data. Since the data set used to perform the experiments described in the paper were not open source, we signed an agreement not to disclose the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMirThqaog4o"
      },
      "source": [
        "!gdown --id [id of your zipped dataser] -O igbodata.zip #This downloads the zipped data set and saves as igbodata.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG-2zyd3cPE9"
      },
      "source": [
        "#To extract the zipped data set\r\n",
        "with zipfile.ZipFile(\"igbodata.zip\",\"r\") as zip_ref:\r\n",
        "    zip_ref.extractall(\"./IgboAudio\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmsH7ePKIAqB"
      },
      "source": [
        "## Train-test-valid split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjffSWaaHk8y"
      },
      "source": [
        "Our zipped data set only has train and test folders. So we need to get the valid data set. In order to boost generalization, we took 70% of the valid dataset from train part and 30% from test part. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4p8XxzbU9K0"
      },
      "source": [
        "#To get the valid indices\r\n",
        "random.seed(123)\r\n",
        "max_words=10\r\n",
        "with open('./IgboAudio/data/train/data.txt', newline='',encoding='UTF-8') as f:\r\n",
        "      tr_data= f.readlines()\r\n",
        "      tr_data = [tr_data[i] for i in range(len(tr_data)) if len((tr_data[i].split('|')[1]).strip().split(' ')) <=max_words]\r\n",
        " \r\n",
        "v = 10000  #samples for valid. Change as you want.\r\n",
        "v_train = 7000 #how many to take from train data\r\n",
        "v_test=3000 #how many to take from test data\r\n",
        "patience=50 #our patience\r\n",
        "BATCH_MULTIPLIER = 7\r\n",
        "test_list = [i for i in range(len(tr_data))]\r\n",
        "valid_indices_train = random.choices(test_list, k=v_train)\r\n",
        "print(f\"Valid indices from train \\n {valid_indices_train}\")\r\n",
        "\r\n",
        "with open('./IgboAudio/data/test/data.txt', newline='',encoding='UTF-8') as f:\r\n",
        "      t_data= f.readlines()\r\n",
        "      t_data = [t_data[i] for i in range(len(t_data))]\r\n",
        "test_list = [i for i in range(len(t_data))]\r\n",
        "valid_indices_test = random.choices(test_list, k=v_test)\r\n",
        "print(f\"Valid indices from test \\n {valid_indices_test}\")\r\n",
        " \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKStcbmUIHDa"
      },
      "source": [
        "We defined a function that based on the type ('train', 'test', 'valid'), generates a list of directories to the audio files, as well as their utterances.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnMlym3vMqCQ"
      },
      "source": [
        "def get_data(datatype): # can be either train or test. Any other format will throw an error.\r\n",
        " \r\n",
        "  if datatype == \"train\":\r\n",
        "    with open('./IgboAudio/data/{}/data.txt'.format(datatype), 'r') as f:\r\n",
        "      data = f.readlines()\r\n",
        "      \r\n",
        "    train_data = [data[i] for i in range(len(data)) if i not in valid_indices_train and len((data[i].split('|')[1]).strip().split(' ')) <=max_words]\r\n",
        "    print(f\"Length of train data: {len(train_data)}\")\r\n",
        "    return train_data\r\n",
        "\r\n",
        "  if datatype == \"test\":\r\n",
        "    with open('./IgboAudio/data/{}/data.txt'.format(datatype), 'r') as f:\r\n",
        "      data = f.readlines()\r\n",
        "      \r\n",
        "    test_data = [data[i] for i in range(len(data)) if i not in valid_indices_test and len((data[i].split('|')[1]).strip().split(' ')) <= max_words]\r\n",
        "    print(f\"Length of test data: {len(test_data)}\")\r\n",
        "    return test_data\r\n",
        "\r\n",
        "  if datatype==\"valid\": # then we should get out some for valid\r\n",
        "     with open('./IgboAudio/data/train/data.txt', 'r') as f:\r\n",
        "      with open('./IgboAudio/data/test/data.txt', 'r') as ft:\r\n",
        "        data = f.readlines()\r\n",
        "        t_data = ft.readlines()\r\n",
        "      \r\n",
        "        v_train_data = [data[i] for i in range(len(data)) if i in valid_indices_train and len((data[i].split('|')[1]).strip().split(' ')) <= max_words]\r\n",
        "        v_test_data = [t_data[i] for i in range(len(t_data)) if i in valid_indices_test and len((tr_data[i].split('|')[1]).strip().split(' ')) <= max_words]\r\n",
        "        val_data = v_train_data + v_test_data\r\n",
        "        print(f\"Length of validation data: {len(val_data)}\")\r\n",
        "        return val_data\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXpQvXynIeHe"
      },
      "source": [
        "## Dataloader class creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6yu6PBMIiG5"
      },
      "source": [
        "Here, we create our special IgboASR dataset that can be used in a torch DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN14HWAaXh-G"
      },
      "source": [
        "import torchaudio\r\n",
        "from torch import Tensor\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torchaudio.datasets.utils import (\r\n",
        "    download_url,\r\n",
        "    extract_archive,\r\n",
        "    walk_files,\r\n",
        ")\r\n",
        "\r\n",
        "def load_audio_item(d: list):\r\n",
        "    d=d.split(\"|\")\r\n",
        "    utterance = d[1].strip()\r\n",
        "    wav_path =d[0]\r\n",
        "    wav_path = re.sub(r'\\\\','/',wav_path)\r\n",
        "    wav_path = wav_path+'.wav'\r\n",
        "    \r\n",
        "    \r\n",
        "    wav_path = wav_path.replace(\"./data/\",'./IgboAudio/data/')\r\n",
        "  \r\n",
        "    \r\n",
        "    #wav_path=os.path.normpath(wav_path)\r\n",
        "    \r\n",
        "    # Load audio\r\n",
        "    waveform, sample_rate = torchaudio.load(wav_path)\r\n",
        "    #print(wav_path)\r\n",
        "    return (waveform, \r\n",
        "        sample_rate,\r\n",
        "        utterance\r\n",
        "    )\r\n",
        "\r\n",
        "\r\n",
        "class IgboASR(torch.utils.data.Dataset):\r\n",
        "    \"\"\"Create a Dataset for Igbo ASR.\r\n",
        "    Args:\r\n",
        "    data_type could be either 'test', 'train' or 'valid'\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, data_type):\r\n",
        "\r\n",
        "      \"\"\"data_type could be either 'test', 'train' or 'valid' \"\"\"\r\n",
        "      self.data = get_data(data_type)\r\n",
        "\r\n",
        "    def __getitem__(self, n: int):\r\n",
        "      \"\"\"Load the n-th sample from the dataset.\r\n",
        "\r\n",
        "      Args:\r\n",
        "          n (int): The index of the sample to be loaded\r\n",
        "\r\n",
        "      Returns:\r\n",
        "          tuple: ``(waveform, sample_rate, utterance)``\r\n",
        "      \"\"\"\r\n",
        "      fileid = self.data[n]\r\n",
        "      return load_audio_item(fileid)\r\n",
        "\r\n",
        "\r\n",
        "    def __len__(self) -> int:\r\n",
        "      return len(self.data)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1fXgsDQmK09"
      },
      "source": [
        "## Taking care of accents and diacritics (for Fon)\r\n",
        "\r\n",
        "since we added diacritics and accents to the model (as described in our paper), this bit of code was written to take care of the mapping: from character to number. The aim of this code is that a character and its accent will be taken as one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWU5n4f8DWUO"
      },
      "source": [
        "accent_code = [b'\\\\u0301',b'\\\\u0300',b'\\\\u0306',b'\\\\u0308',b'\\\\u0303']\r\n",
        "alpha = {'ɔ':0,'ɛ':5}\r\n",
        "accents = {b'\\\\u0301':1,b'\\\\u0300':2,b'\\\\u0306':3,b'\\\\u0308':4,b'\\\\u0303':5}\r\n",
        "mapping={\r\n",
        "    1:'ɔ́',2:'ɔ̀',3:'ɔ̆',6:'έ',7:'ὲ',8:'ɛ̆'\r\n",
        "}\r\n",
        "#we are following the idea that the composition gives the letter first followed by the sign(accent)\r\n",
        "def get_better_mapping(text):\r\n",
        "  t_arr = [t for t in text]\r\n",
        "  s=[]\r\n",
        "  for i in range(len(t_arr)):\r\n",
        "    if t_arr[i].encode(\"unicode_escape\") in accent_code:\r\n",
        "      to_check = s[-1]\r\n",
        "      try:\r\n",
        "        val = mapping[alpha[to_check] + accents[t_arr[i].encode(\"unicode_escape\")]]\r\n",
        "        s.pop()\r\n",
        "        s.append(val)\r\n",
        "      except KeyError:\r\n",
        "        #print(\"Could not find for {} in sentence {} | Proceeding with default.\".format(t_arr[i],text))\r\n",
        "        print(\"\")\r\n",
        "      \r\n",
        "    else: \r\n",
        "      s.append(t_arr[i])\r\n",
        "  return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSKHvy8DmOCQ"
      },
      "source": [
        "## Setting up your metrics\r\n",
        "Here we define the functions to calculate the error of the model using the metrics:\r\n",
        "1.   WER\r\n",
        "2.   CER\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvH3X5LLJpKk"
      },
      "source": [
        "\r\n",
        "def avg_wer(wer_scores, combined_ref_len):\r\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\r\n",
        "\r\n",
        "\r\n",
        "def _levenshtein_distance(ref, hyp):\r\n",
        "    \"\"\"Levenshtein distance is a string metric for measuring the difference\r\n",
        "    between two sequences. Informally, the levenshtein disctance is defined as\r\n",
        "    the minimum number of single-character edits (substitutions, insertions or\r\n",
        "    deletions) required to change one word into the other. We can naturally\r\n",
        "    extend the edits to word level when calculate levenshtein disctance for\r\n",
        "    two sentences.\r\n",
        "    \"\"\"\r\n",
        "    m = len(ref)\r\n",
        "    n = len(hyp)\r\n",
        "\r\n",
        "    # special case\r\n",
        "    if ref == hyp:\r\n",
        "        return 0\r\n",
        "    if m == 0:\r\n",
        "        return n\r\n",
        "    if n == 0:\r\n",
        "        return m\r\n",
        "\r\n",
        "    if m < n:\r\n",
        "        ref, hyp = hyp, ref\r\n",
        "        m, n = n, m\r\n",
        "\r\n",
        "    # use O(min(m, n)) space\r\n",
        "    distance = np.zeros((2, n + 1), dtype=np.int32)\r\n",
        "\r\n",
        "    # initialize distance matrix\r\n",
        "    for j in range(0,n + 1):\r\n",
        "        distance[0][j] = j\r\n",
        "\r\n",
        "    # calculate levenshtein distance\r\n",
        "    for i in range(1, m + 1):\r\n",
        "        prev_row_idx = (i - 1) % 2\r\n",
        "        cur_row_idx = i % 2\r\n",
        "        distance[cur_row_idx][0] = i\r\n",
        "        for j in range(1, n + 1):\r\n",
        "            if ref[i - 1] == hyp[j - 1]:\r\n",
        "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\r\n",
        "            else:\r\n",
        "                s_num = distance[prev_row_idx][j - 1] + 1\r\n",
        "                i_num = distance[cur_row_idx][j - 1] + 1\r\n",
        "                d_num = distance[prev_row_idx][j] + 1\r\n",
        "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\r\n",
        "\r\n",
        "    return distance[m % 2][n]\r\n",
        "\r\n",
        "\r\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\r\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\r\n",
        "    hypothesis sequence in word-level.\r\n",
        "    :param reference: The reference sentence.\r\n",
        "    :type reference: basestring\r\n",
        "    :param hypothesis: The hypothesis sentence.\r\n",
        "    :type hypothesis: basestring\r\n",
        "    :param ignore_case: Whether case-sensitive or not.\r\n",
        "    :type ignore_case: bool\r\n",
        "    :param delimiter: Delimiter of input sentences.\r\n",
        "    :type delimiter: char\r\n",
        "    :return: Levenshtein distance and word number of reference sentence.\r\n",
        "    :rtype: list\r\n",
        "    \"\"\"\r\n",
        "    if ignore_case == True:\r\n",
        "        reference = reference.lower()\r\n",
        "        hypothesis = hypothesis.lower()\r\n",
        "\r\n",
        "    ref_words = reference.split(delimiter)\r\n",
        "    hyp_words = hypothesis.split(delimiter)\r\n",
        "\r\n",
        "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\r\n",
        "    return float(edit_distance), len(ref_words)\r\n",
        "\r\n",
        "\r\n",
        "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\r\n",
        "    \"\"\"Compute the levenshtein distance between reference sequence and\r\n",
        "    hypothesis sequence in char-level.\r\n",
        "    :param reference: The reference sentence.\r\n",
        "    :type reference: basestring\r\n",
        "    :param hypothesis: The hypothesis sentence.\r\n",
        "    :type hypothesis: basestring\r\n",
        "    :param ignore_case: Whether case-sensitive or not.\r\n",
        "    :type ignore_case: bool\r\n",
        "    :param remove_space: Whether remove internal space characters\r\n",
        "    :type remove_space: bool\r\n",
        "    :return: Levenshtein distance and length of reference sentence.\r\n",
        "    :rtype: list\r\n",
        "    \"\"\"\r\n",
        "    if ignore_case == True:\r\n",
        "        reference = reference.lower()\r\n",
        "        hypothesis = hypothesis.lower()\r\n",
        "\r\n",
        "    join_char = ' '\r\n",
        "    if remove_space == True:\r\n",
        "        join_char = ''\r\n",
        "\r\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\r\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\r\n",
        "\r\n",
        "    edit_distance = _levenshtein_distance(reference, hypothesis)\r\n",
        "    return float(edit_distance), len(reference)\r\n",
        "\r\n",
        "\r\n",
        "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\r\n",
        "    \"\"\"Calculate word error rate (WER). WER compares reference text and\r\n",
        "    hypothesis text in word-level. WER is defined as:\r\n",
        "    .. math::\r\n",
        "        WER = (Sw + Dw + Iw) / Nw\r\n",
        "    where\r\n",
        "    .. code-block:: text\r\n",
        "        Sw is the number of words subsituted,\r\n",
        "        Dw is the number of words deleted,\r\n",
        "        Iw is the number of words inserted,\r\n",
        "        Nw is the number of words in the reference\r\n",
        "    We can use levenshtein distance to calculate WER. Please draw an attention\r\n",
        "    that empty items will be removed when splitting sentences by delimiter.\r\n",
        "    :param reference: The reference sentence.\r\n",
        "    :type reference: basestring\r\n",
        "    :param hypothesis: The hypothesis sentence.\r\n",
        "    :type hypothesis: basestring\r\n",
        "    :param ignore_case: Whether case-sensitive or not.\r\n",
        "    :type ignore_case: bool\r\n",
        "    :param delimiter: Delimiter of input sentences.\r\n",
        "    :type delimiter: char\r\n",
        "    :return: Word error rate.\r\n",
        "    :rtype: float\r\n",
        "    :raises ValueError: If word number of reference is zero.\r\n",
        "    \"\"\"\r\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\r\n",
        "                                         delimiter)\r\n",
        "\r\n",
        "    if ref_len == 0:\r\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\r\n",
        "\r\n",
        "    wer = float(edit_distance) / ref_len\r\n",
        "    return wer\r\n",
        "\r\n",
        "\r\n",
        "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\r\n",
        "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\r\n",
        "    hypothesis text in char-level. CER is defined as:\r\n",
        "    .. math::\r\n",
        "        CER = (Sc + Dc + Ic) / Nc\r\n",
        "    where\r\n",
        "    .. code-block:: text\r\n",
        "        Sc is the number of characters substituted,\r\n",
        "        Dc is the number of characters deleted,\r\n",
        "        Ic is the number of characters inserted\r\n",
        "        Nc is the number of characters in the reference\r\n",
        "    We can use levenshtein distance to calculate CER. Chinese input should be\r\n",
        "    encoded to unicode. Please draw an attention that the leading and tailing\r\n",
        "    space characters will be truncated and multiple consecutive space\r\n",
        "    characters in a sentence will be replaced by one space character.\r\n",
        "    :param reference: The reference sentence.\r\n",
        "    :type reference: basestring\r\n",
        "    :param hypothesis: The hypothesis sentence.\r\n",
        "    :type hypothesis: basestring\r\n",
        "    :param ignore_case: Whether case-sensitive or not.\r\n",
        "    :type ignore_case: bool\r\n",
        "    :param remove_space: Whether remove internal space characters\r\n",
        "    :type remove_space: bool\r\n",
        "    :return: Character error rate.\r\n",
        "    :rtype: float\r\n",
        "    :raises ValueError: If the reference length is zero.\r\n",
        "    \"\"\"\r\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\r\n",
        "                                         remove_space)\r\n",
        "\r\n",
        "    if ref_len == 0:\r\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\r\n",
        "\r\n",
        "    cer = float(edit_distance) / ref_len\r\n",
        "    return cer\r\n",
        "\r\n",
        "class TextTransform:\r\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\r\n",
        "    def __init__(self):\r\n",
        "      #The Igbo alphabet (used for this experiment) consists of the set {a,...,z,space,apostrophe,comma} \r\n",
        "        char_map_str = \"\"\"\r\n",
        "          ' 0\r\n",
        "          <SPACE> 1\r\n",
        "          a 2\r\n",
        "          b 3\r\n",
        "          c 4\r\n",
        "          d 5\r\n",
        "          e 6\r\n",
        "          f 7\r\n",
        "          g 8\r\n",
        "          h 9\r\n",
        "          i 10\r\n",
        "          j 11\r\n",
        "          k 12\r\n",
        "          l 13\r\n",
        "          m 14\r\n",
        "          n 15\r\n",
        "          o 16\r\n",
        "          p 17\r\n",
        "          q 18\r\n",
        "          r 19\r\n",
        "          s 20\r\n",
        "          t 21\r\n",
        "          u 22\r\n",
        "          v 23\r\n",
        "          w 24\r\n",
        "          x 25\r\n",
        "          y 26\r\n",
        "          z 27\r\n",
        "          \"\"\"\r\n",
        "        self.char_map = {}\r\n",
        "        self.index_map = {}\r\n",
        "        for line in char_map_str.strip().split('\\n'):\r\n",
        "            ch, index = line.split()\r\n",
        "            self.char_map[ch] = int(index)\r\n",
        "            self.index_map[int(index)] = ch\r\n",
        "        self.index_map[1] = ' '\r\n",
        "\r\n",
        "    def text_to_int(self, text):\r\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\r\n",
        "        int_sequence = []\r\n",
        "        #text=unicodedata.normalize(\"NFC\",text)\r\n",
        "        for c in text.strip():\r\n",
        "            try:\r\n",
        "              if c == ' ':\r\n",
        "                  ch = 1\r\n",
        "              #elif c =='̀':\r\n",
        "              #    ch=0\r\n",
        "              else:\r\n",
        "                  ch = self.char_map[c]\r\n",
        "            except KeyError:\r\n",
        "              print(\"Error for character {} in this sentence: {}\".format(c,text))\r\n",
        "              ch=0\r\n",
        "            int_sequence.append(ch)\r\n",
        "        return int_sequence\r\n",
        "\r\n",
        "    def int_to_text(self, labels):\r\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\r\n",
        "        string = []\r\n",
        "        for i in labels:\r\n",
        "            string.append(self.index_map[i])\r\n",
        "        return ''.join(string).replace('<SPACE>', ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5GAu7MKJqWf"
      },
      "source": [
        "## Data processing and Greedy Decoder\r\n",
        "\r\n",
        "Here we deifne the data preprocessing part: the mel spectogram, and data augmentation techniques. We also define the GreedyDecoder algorithm which is used at the model output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVJs4Bk8FjjO"
      },
      "source": [
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "test_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "\n",
        "    for waveform,_,utterance in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'test':\n",
        "            spec = test_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train, valid or test')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "    \n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XdSlhAQnDEA"
      },
      "source": [
        "## The Model\n",
        "See [here](https://drive.google.com/file/d/1gT4r1R8Iq_183WkU3l0nNtdy4YYvYHPp/view?usp=sharing) for more explanation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65H1-PCjm-FB"
      },
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batch norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "class BidirectionalLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "\n",
        "        self.BiLSTM = nn.LSTM(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiLSTM(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers1 = nn.Sequential(*[\n",
        "            BidirectionalLSTM(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.birnn_layers2 = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        # print(x.size())\n",
        "        x = self.birnn_layers1(x)\n",
        "        # print(x.size())\n",
        "        x = self.birnn_layers2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuguNEzKnMOn"
      },
      "source": [
        "## The Training and Evaluating Script\r\n",
        "\r\n",
        "We define seperate functions to perform the training, validation and testing. Then we put it together inside a 'main' function.\r\n",
        "\r\n",
        "Here we also made tweaks to allow for saving best model weights to specified path in GDrive and re-training from last saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydkqGeOwnPGY"
      },
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment,valid_loader,best_loss,curr_patience,best_test_loss):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    train_loss=0\n",
        "    batch_train_loss=0\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        output = model(spectrograms)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        #print(\"Loss untouched: \",loss.item())\n",
        "        train_loss += loss.item() / (len(train_loader)*BATCH_MULTIPLIER)\n",
        "        loss.backward()\n",
        "\n",
        "       \n",
        "        if (batch_idx + 1) % BATCH_MULTIPLIER == 0:\n",
        "       \n",
        "            optimizer.step()\n",
        "            #scheduler.step()\n",
        "            iter_meter.step()\n",
        "            #model.zero_grad() #reset gradients\n",
        "            optimizer.zero_grad()\n",
        "            batch_train_loss+=train_loss\n",
        "            train_loss=0\n",
        "        if batch_idx % 300 == 0 or batch_idx == data_len:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(spectrograms), data_len,\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "    experiment['loss'].append((batch_train_loss,iter_meter.get()))\n",
        "    val_loss,val_test_loss = valid(model, device, valid_loader, criterion, epoch, iter_meter, experiment)\n",
        "    if val_loss < best_loss:\n",
        "      curr_patience=0\n",
        "      best_loss = val_loss\n",
        "      #save model dicts\n",
        "      torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'val_loss':val_loss\n",
        "              }, model_path)\n",
        "\n",
        "    else:\n",
        "      curr_patience+=1\n",
        "      print(\"...No improvement in validation WER from {}...\".format(best_loss))\n",
        "    if val_test_loss < best_test_loss:\n",
        "      print(\"Improvement in main loss. Saving model weights to model_path_loss \")\n",
        "      curr_patience=0\n",
        "      best_test_loss = val_test_loss\n",
        "      #save model dicts\n",
        "      torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'val_loss':val_test_loss\n",
        "              }, model_path_loss)\n",
        "\n",
        "    \n",
        "    return best_loss, curr_patience,best_test_loss\n",
        "\n",
        "\n",
        "def valid(model, device, test_loader, criterion, epoch, iter_meter, experiment):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "    experiment['val_loss'].append((test_loss, iter_meter.get()))\n",
        "    experiment['cer'].append((avg_cer, iter_meter.get()))\n",
        "    experiment['wer'].append((avg_wer, iter_meter.get()))\n",
        "\n",
        "    print('Valid set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "    return avg_wer,test_loss\n",
        "\n",
        "    \n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter, experiment):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "    \n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "def main(learning_rate, batch_size, epochs,experiment, disabled=True):\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": 7,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 29,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    \n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print(\"DEVICE: {}\".format(device))\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        print(\"Making dir of /data\")\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "\n",
        "    train_dataset = IgboASR(\"train\")\n",
        "    valid_dataset = IgboASR(\"valid\")\n",
        "    test_dataset = IgboASR(\"test\")\n",
        "    \n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'test'),\n",
        "                                **kwargs)\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "    print(model)\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=28,zero_infinity=True).to(device)\n",
        "    #scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
        "    #                                        steps_per_epoch=batch_size*BATCH_MULTIPLIER,\n",
        "     #                                       epochs=hparams['epochs'],anneal_strategy=\"linear\")\n",
        "    scheduler=None\n",
        "\n",
        "\n",
        "    iter_meter = IterMeter()\n",
        "    best_loss=1000\n",
        "    best_test_loss=1000\n",
        "    \n",
        "    curr_patience=0\n",
        "    \"\"\"\n",
        "    #If your Colab crashes mid-way, uncomment this code\n",
        "\n",
        "    #If loading checkpoint for best model on WER, comment the block below\n",
        "    checkpoint = torch.load(model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch_saved = checkpoint['epoch']\n",
        "    best_loss = checkpoint['val_loss']\n",
        "    best_test_loss=1000\n",
        "    \"\"\"\n",
        "    #if loading checkpoint for best model on Loss, comment the block above\n",
        "    checkpoint = torch.load(model_path_loss)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch_saved = checkpoint['epoch']\n",
        "    best_test_loss = checkpoint['val_loss']\n",
        "    best_loss=1000\n",
        "    \n",
        "\n",
        "    for epoch in range(epoch_saved, epochs + 1):\n",
        "        best_loss,curr_patience,best_test_loss = train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment,valid_loader,best_loss,curr_patience,best_test_loss)\n",
        "        #valid(model, device, valid_loader, criterion, epoch, iter_meter, experiment)\n",
        "        if curr_patience==patience:\n",
        "          print(\"Early stopping with patience of {}\".format(patience))\n",
        "          break\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        best_loss,curr_patience,best_test_loss = train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter, experiment,valid_loader,best_loss,curr_patience,best_test_loss)\n",
        "        #valid(model, device, valid_loader, criterion, epoch, iter_meter, experiment)\n",
        "        if curr_patience==patience:\n",
        "          print(\"Early stopping with patience of {}\".format(patience))\n",
        "          break\n",
        "    \"\"\"\n",
        "    print(\"Evaluating on Test data:\")\n",
        "    test(model, device, test_loader, criterion, epoch, iter_meter, experiment)\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXvlWZeVpXfX"
      },
      "source": [
        "## Running Experiment\n",
        "\n",
        "Here you choose the learning rate, batch size and epochs. Then you run the cell and your model starts training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZodve8PGKfS"
      },
      "source": [
        "\"\"\"\n",
        "experiment logs the metrics of the model while training. each array element is a tuple (metric,step).\n",
        "For example, we may have loss= [(loss:1.0, step:2)]\n",
        "\"\"\"\n",
        "experiment={\n",
        "    'loss':[],\n",
        "    'val_loss':[],\n",
        "    'cer':[],\n",
        "    'wer':[]\n",
        "\n",
        "}\n",
        "\n",
        "learning_rate = 0.001\n",
        "batch_size = 6\n",
        "epochs = 1000\n",
        "\n",
        "print(\"Using BATCH SIZE -> {} and multiplier -> {}\".format(batch_size,BATCH_MULTIPLIER))\n",
        "main(learning_rate, batch_size, epochs, experiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3Xpwu5iSNKu"
      },
      "source": [
        "\r\n",
        "Below is for Debugging\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WVHNBlTLE7c"
      },
      "source": [
        "\r\n",
        "txt = get_better_mapping('ɖɔ̆lɔ̆àyìɔ̆zέn')\r\n",
        "print(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucfQX3qN21az"
      },
      "source": [
        "txt = unicodedata.normalize(\"NFC\",'ɛ̃')\r\n",
        "#txt='Bäume'\r\n",
        "c=0\r\n",
        "print([i for i in txt])\r\n",
        "for t in txt:\r\n",
        "  c+=1\r\n",
        "\r\n",
        "  print(t.encode(\"unicode_escape\"))\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}